<!DOCTYPE html>
<!-- saved from url=(0027)https://svraster.github.io/ -->
<html class="fontawesome-i2svg-active fontawesome-i2svg-complete"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <meta name="description" content="LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields">
  <meta name="keywords" content="Sparse view reconstruction, inverse rendering, large reconstruction model">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="icon" type="image/png" href="./index_files/icon.png">
  <title>LIRM</title>

  <style type="text/css">svg:not(:root).svg-inline--fa{overflow:visible}.svg-inline--fa{display:inline-block;font-size:inherit;height:1em;overflow:visible;vertical-align:-.125em}.svg-inline--fa.fa-lg{vertical-align:-.225em}.svg-inline--fa.fa-w-1{width:.0625em}.svg-inline--fa.fa-w-2{width:.125em}.svg-inline--fa.fa-w-3{width:.1875em}.svg-inline--fa.fa-w-4{width:.25em}.svg-inline--fa.fa-w-5{width:.3125em}.svg-inline--fa.fa-w-6{width:.375em}.svg-inline--fa.fa-w-7{width:.4375em}.svg-inline--fa.fa-w-8{width:.5em}.svg-inline--fa.fa-w-9{width:.5625em}.svg-inline--fa.fa-w-10{width:.625em}.svg-inline--fa.fa-w-11{width:.6875em}.svg-inline--fa.fa-w-12{width:.75em}.svg-inline--fa.fa-w-13{width:.8125em}.svg-inline--fa.fa-w-14{width:.875em}.svg-inline--fa.fa-w-15{width:.9375em}.svg-inline--fa.fa-w-16{width:1em}.svg-inline--fa.fa-w-17{width:1.0625em}.svg-inline--fa.fa-w-18{width:1.125em}.svg-inline--fa.fa-w-19{width:1.1875em}.svg-inline--fa.fa-w-20{width:1.25em}.svg-inline--fa.fa-pull-left{margin-right:.3em;width:auto}.svg-inline--fa.fa-pull-right{margin-left:.3em;width:auto}.svg-inline--fa.fa-border{height:1.5em}.svg-inline--fa.fa-li{width:2em}.svg-inline--fa.fa-fw{width:1.25em}.fa-layers svg.svg-inline--fa{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.fa-layers{display:inline-block;height:1em;position:relative;text-align:center;vertical-align:-.125em;width:1em}.fa-layers svg.svg-inline--fa{-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter,.fa-layers-text{display:inline-block;position:absolute;text-align:center}.fa-layers-text{left:50%;top:50%;-webkit-transform:translate(-50%,-50%);transform:translate(-50%,-50%);-webkit-transform-origin:center center;transform-origin:center center}.fa-layers-counter{background-color:#ff253a;border-radius:1em;-webkit-box-sizing:border-box;box-sizing:border-box;color:#fff;height:1.5em;line-height:1;max-width:5em;min-width:1.5em;overflow:hidden;padding:.25em;right:0;text-overflow:ellipsis;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-bottom-right{bottom:0;right:0;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom right;transform-origin:bottom right}.fa-layers-bottom-left{bottom:0;left:0;right:auto;top:auto;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:bottom left;transform-origin:bottom left}.fa-layers-top-right{right:0;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top right;transform-origin:top right}.fa-layers-top-left{left:0;right:auto;top:0;-webkit-transform:scale(.25);transform:scale(.25);-webkit-transform-origin:top left;transform-origin:top left}.fa-lg{font-size:1.3333333333em;line-height:.75em;vertical-align:-.0667em}.fa-xs{font-size:.75em}.fa-sm{font-size:.875em}.fa-1x{font-size:1em}.fa-2x{font-size:2em}.fa-3x{font-size:3em}.fa-4x{font-size:4em}.fa-5x{font-size:5em}.fa-6x{font-size:6em}.fa-7x{font-size:7em}.fa-8x{font-size:8em}.fa-9x{font-size:9em}.fa-10x{font-size:10em}.fa-fw{text-align:center;width:1.25em}.fa-ul{list-style-type:none;margin-left:2.5em;padding-left:0}.fa-ul>li{position:relative}.fa-li{left:-2em;position:absolute;text-align:center;width:2em;line-height:inherit}.fa-border{border:solid .08em #eee;border-radius:.1em;padding:.2em .25em .15em}.fa-pull-left{float:left}.fa-pull-right{float:right}.fa.fa-pull-left,.fab.fa-pull-left,.fal.fa-pull-left,.far.fa-pull-left,.fas.fa-pull-left{margin-right:.3em}.fa.fa-pull-right,.fab.fa-pull-right,.fal.fa-pull-right,.far.fa-pull-right,.fas.fa-pull-right{margin-left:.3em}.fa-spin{-webkit-animation:fa-spin 2s infinite linear;animation:fa-spin 2s infinite linear}.fa-pulse{-webkit-animation:fa-spin 1s infinite steps(8);animation:fa-spin 1s infinite steps(8)}@-webkit-keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}@keyframes fa-spin{0%{-webkit-transform:rotate(0);transform:rotate(0)}100%{-webkit-transform:rotate(360deg);transform:rotate(360deg)}}.fa-rotate-90{-webkit-transform:rotate(90deg);transform:rotate(90deg)}.fa-rotate-180{-webkit-transform:rotate(180deg);transform:rotate(180deg)}.fa-rotate-270{-webkit-transform:rotate(270deg);transform:rotate(270deg)}.fa-flip-horizontal{-webkit-transform:scale(-1,1);transform:scale(-1,1)}.fa-flip-vertical{-webkit-transform:scale(1,-1);transform:scale(1,-1)}.fa-flip-both,.fa-flip-horizontal.fa-flip-vertical{-webkit-transform:scale(-1,-1);transform:scale(-1,-1)}:root .fa-flip-both,:root .fa-flip-horizontal,:root .fa-flip-vertical,:root .fa-rotate-180,:root .fa-rotate-270,:root .fa-rotate-90{-webkit-filter:none;filter:none}.fa-stack{display:inline-block;height:2em;position:relative;width:2.5em}.fa-stack-1x,.fa-stack-2x{bottom:0;left:0;margin:auto;position:absolute;right:0;top:0}.svg-inline--fa.fa-stack-1x{height:1em;width:1.25em}.svg-inline--fa.fa-stack-2x{height:2em;width:2.5em}.fa-inverse{color:#fff}.sr-only{border:0;clip:rect(0,0,0,0);height:1px;margin:-1px;overflow:hidden;padding:0;position:absolute;width:1px}.sr-only-focusable:active,.sr-only-focusable:focus{clip:auto;height:auto;margin:0;overflow:visible;position:static;width:auto}.svg-inline--fa .fa-primary{fill:var(--fa-primary-color,currentColor);opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa .fa-secondary{fill:var(--fa-secondary-color,currentColor);opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-primary{opacity:.4;opacity:var(--fa-secondary-opacity,.4)}.svg-inline--fa.fa-swap-opacity .fa-secondary{opacity:1;opacity:var(--fa-primary-opacity,1)}.svg-inline--fa mask .fa-primary,.svg-inline--fa mask .fa-secondary{fill:#000}.fad.fa-inverse{color:#fff}</style><link href="./SVRaster_files/css" rel="stylesheet">

  <link rel="stylesheet" href="./SVRaster_files/bulma.min.css">
  <link rel="stylesheet" href="./SVRaster_files/bulma-carousel.min.css">
  <link rel="stylesheet" href="./SVRaster_files/bulma-slider.min.css">
  <link rel="stylesheet" href="./SVRaster_files/fontawesome.all.min.css">
  <link rel="stylesheet" href="./SVRaster_files/academicons.min.css">
  <link rel="stylesheet" href="./SVRaster_files/index.css">

  <script src="./SVRaster_files/jquery.min.js"></script>
  <script defer="" src="./SVRaster_files/fontawesome.all.min.js"></script>
  <script src="./SVRaster_files/bulma-carousel.min.js"></script>
  <script src="./SVRaster_files/bulma-slider.min.js"></script>
  <script src="./SVRaster_files/index.js"></script>
  <script src="./SVRaster_files/script.js"></script>

</head>

<body>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h2 class="title is-2 publication-title">
              <span style="color: #76b900; font-weight: bolder;">L</span>arge
              <span style="color: #76b900; font-weight: bolder;">I</span>nverse
              <span style="color: #76b900; font-weight: bolder;">R</span>endering
              <span style="color: #76b900; font-weight: bolder;">M</span>odel for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields
            </h2>

            <div style="line-height:50%;">
              <br>
            </div>

            <div class="subtitle is-size-4">
              CVPR 2025
            </div>

            <div style="line-height:20%;">
              <br>
            </div>

            <div class="row">
              <div class="col-md-8 col-md-offset-2">
                  <img src="./index_files/teaser.png" class="img-responsive" alt="overview"><br>
              </div>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="https://sites.google.com/view/zhengqinli">Zhengqin Li</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://wdilin.github.io/">Dilin Wang</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/ka-chen-ab116a31/details/experience/">Ka Chen</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://lvzhaoyang.github.io/">Zhaoyang Lv</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.monkeyoverflow.com/about">Thu Nguyen-Phuoc</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/milimlee/">Milim Lee</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://jbhuang0604.github.io/">Jia-Bin Huang</a><sup>12</sup>
              </span>
              <span class="author-block">
                <a href="https://leixiao-ubc.github.io/">Lei Xiao</a><sup>1</sup>
              </span>
            </div>

            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <a href="https://holmes969.github.io/">Cheng Zhang</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://mike323zyf.github.io/">Yufeng Zhu</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.linkedin.com/in/carl-s-marshall/">Carl S. Marshall</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://www.carlyuheng.com/">Yuheng Ren</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://rapiderobot.bitbucket.io/">Richard Newcombe</a><sup>1</sup>
              </span>
              <span class="author-block">
                <a href="https://flycooler.com/">Zhao Dong</a><sup>1</sup>
              </span>
            </div>

            <div class="subtitle is-size-6">
              <sup>1</sup> Meta Reality Labs, Research &nbsp;
              <sup>2</sup> University of Maryland, College Park &nbsp;
            </div>


            <div class="column has-text-centered">
              <span class="link-block">
                <a href="https://arxiv.org/abs/2504.20026" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./index_files/arxiv.png" height="60px">
                  </span>
                  <span>Arxiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1tPbj8j3iqq1a4J6q6aWH3XAXcSiCjkDb/view?usp=drive_link" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <img src="./index_files/video.png" height="80px">
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://lzqsd.github.io/LIRM.github.io/" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <svg class="svg-inline--fa fa-github fa-w-16" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github"></i> Font Awesome fontawesome.com -->
                  </span>
                  <span>Code (Coming soon)</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class=" carousel results-carousel">
          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_1.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_2.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_3.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_4.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_5.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_6.mp4" type="video/mp4">
            </video>
          </div>

          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_7.mp4" type="video/mp4">
            </video>
          </div>
          
          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_8.mp4" type="video/mp4">
            </video>
          </div>
          
          <div class="item">
            <video controls="" muted="" loop="true" autoplay="true" playsinline="" height="100%">
              <source src="./index_files/inverse_9.mp4" type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Overview</h2>
          <div class="content has-text-justified">
            <p>
              We present Large Inverse Rendering Model (LIRM), a transformer architecture that jointly reconstructs high-quality shape, materials,
              and radiance fields with view-dependent effects in less than a second. Our model builds upon the recent Large Reconstruction Models (LRMs)
              that achieve state-of-the-art sparse-view reconstruction quality. However, existing LRMs struggle to reconstruct unseen parts accurately
              and cannot recover glossy appearance or generate relightable 3D contents that can be consumed by standard Graphics engines.
              To address these limitations, we make three key technical contributions to build a more practical multi-view 3D reconstruction framework.
              First, we introduce an update model that allows us to progressively add more input views to improve our reconstruction.
              Second, we propose a hexa-plane neural SDF representation to better recover detailed textures, geometry and material parameters.
              Third, we develop a novel neural directional-embedding mechanism to handle view-dependent effects.
              Trained on a large-scale shape and material dataset with a tailored coarse-to-fine training scheme, our model achieves compelling results.
              It compares favorably to optimization-based dense-view inverse rendering methods in terms of geometry and relighting accuracy,
              while requiring only a fraction of the inference time.
            </p>
          </div>
          <div class="content has-text-justified">
            <strong>Hilights</strong>
            <p class="text-justify">
              <ul>
                <li>
                  Feedforward sparse-view inverse rendering of shape, spatially-varying materials, view-dependent radiance fields in less than a second,
                  with quality on par with or even better than SOTA optimization-based methods.
                </li>
                <li>
                    An update model that supports progressive reconstruction with more input views.
                </li>
                <li>
                    A novel hexa-plane neural SDF representation that better preserves geometry and texture details.
                </li>
                <li>
                    Adopting neural directional-embedding (NDE) mechanism to handle view-dependent effects.
                </li>
              </ul>
            </p>
          </div>
        </div>
      </div>
      <!--/ Abstract. -->
      <div class="has-text-centered">
        <img src="./index_files/network.png">
      </div>
      <div class="content has-text-justified">
        <strong>Model architecture</strong>
        <p class="text-justify">
          The network architecture of LIRM is shown above. The inputs are masked images, background images to provide more lighting information,
          and Plucker rays that encodes camera intrinsics and extrinsics. These 3 images are concatenated together and turned into tokens through a simple linear layer.
          These tokens are sent to a transformer that consists of 24 self-attention block to update hexa-plane tokens and NDE tokens.
          We decode the 2 kinds of tokens into the hexa-plane representation and NDE panoramas through linear layers,
          which can be used to render view dependent radiance fields and BRDF parameters through neural volume rendering.
          The decoded SDF volume can be used to extract accurate triangular mesh through standard marching cube.
        </p>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Update Model For Progressive Reconstruction </h2>
          <div class="content has-text-justified">
            <video id="v0" width="100%" autoplay="true" loop="true" muted="" controls="">
              <source src="index_files/update.mp4" type="video/mp4">
            </video>
            <p class="text-justify">
              LIRM can progressively improve reconstruction quality with more input images, enabling interactive reconstruction.
              The above shows an example where the first set of 4 input images only cover the front side of the object
              and the second set of 4 images only cover the back side.
              With the first set of 4 input images, our LIRM onlyreconstructs the front side of the object accurately.
              After taking the second set of inputs, our network updates the tri-plane prediction to obtain high-quality reconstruction of the full 3D object.
            </p>
            <video id="v1" width="100%" autoplay="true" loop="true" muted="" controls="">
              <source src="index_files/changing_scene.mp4" type="video/mp4">
            </video>
            <p class="text-justify">
              Our update model can generalize to challenging changing scene scenarios without any fine-tuning. In the example above,
              we first use LIRM to reconstruct the teddy bear from 4 input images. Then we add a small figurine behind the teddy bear
              and take another 4 images as the second set of inputs. Our model successfully utilizes new input images to
              reconstruct the added figurine while keeping the front side of the teddy bear unchanged, even though the front side is not observed
              in these new inputs.
            </p>
          </div>
        </div>
      </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Hexa-plane VS. Tri-plane</h2>
        </div>
      </div>

      <div class="columns is-full-width">
        <div class="column is-full-width">
          <div id="example1" class="bal-container-small">
            <div class="bal-after">
              <img src="./index_files/double_1.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Hexa-plane
              </div>
            </div>
            <div class="bal-before" style="width: 50%;">
              <div class="bal-before-inset" style="width: 660px;">
                <img src="./index_files/single_1.png">
                <div class="bal-beforePosition beforeLabel">
                  Tri-plane
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left: 50%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>

        <div class="column is-full-width">
          <div id="example2" class="bal-container-small">
            <div class="bal-after">
              <img src="./index_files/double_2.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Hexa-plane
              </div>
            </div>
            <div class="bal-before" style="width: 50%;">
              <div class="bal-before-inset" style="width: 660px;">
                <img src="./index_files/single_2.png">
                <div class="bal-beforePosition beforeLabel">
                  Tri-plane
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left: 50%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>

        <div class="column is-full-width">
          <div id="example3" class="bal-container-small">
            <div class="bal-after">
              <img src="./index_files/double_3.png">
              <div class="bal-afterPosition afterLabel" style="z-index:1;">
                Hexa-plane
              </div>
            </div>
            <div class="bal-before" style="width: 50%;">
              <div class="bal-before-inset" style="width: 660px;">
                <img src="./index_files/single_3.png">
                <div class="bal-beforePosition beforeLabel">
                  Tri-plane
                </div>
              </div>
            </div>
            <div class="bal-handle" style="left: 50%;">
              <span class="handle-left-arrow"></span>
              <span class="handle-right-arrow"></span>
            </div>
          </div>
        </div>
      </div>
      <div class="column is-full-width">
        <div class="content has-text-justified">
          <p class="text-justify">
            While tri-plane-based 3D representation can achieve highly detailed reconstruction in most scenarios,
            we observe that it struggles when both sides of an object contain complex but different textures,
            as shown above. This limitation arises from using a single feature plane to represent both sides of textures.
            Therefore, we adopt a hexa-plane representation where we use 6 planes to divide the bounding box into 8 volumes,
            each with its own tri-plane. This simple modification prevents texture patterns from "leaking" to the other side,
            as shown in the above examples from the
            <a href="https://research.google/blog/scanned-objects-by-google-research-a-dataset-of-3d-scanned-common-household-items/">
              Google Scanned Object
            </a>
            dataset. Note that we reduce the resolution of hexa-plane so that the total computational cost remains roughly the same.
          </p>
        </div>
      </div>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">Adopting NDE for View-dependent Radiance Fields</h2>
          <div class="content has-text-justified">
            <video id="v0" width="100%" autoplay="true" loop="true" muted="" controls="">
              <source src="index_files/nde.mp4" type="video/mp4">
            </video>
            <p class="text-justify">
                Existing feedforward sparse-view reconstruction methods neglect view-dependent effects. We tackle this problem by
                including neural directional encoding (NDE) from Wu et al. into LIRM. Experiments on both synthetic and real data show
                that LIRM can better recover specular highlights visible in the input images, as shown in the example above.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-full-width">
          <h2 class="title is-3">High-quality Sparse-view Inverse Rendering</h2>
          <div class="content has-text-justified">
            <div class="row">
              <div class="col-md-8 col-md-offset-2">
                  <img src="./index_files/stanford_orb.png" class="img-responsive" alt="overview"><br>
              </div>
              <div class="col-md-8 col-md-offset-2">
                <img src="./index_files/stanford_orb_quant.png" class="img-responsive" alt="overview"><br>
              </div>
            </div>
            <p class="text-justify">
              By combining the above technical components, LIRM achieves impressive high-quality sparse-view inverse rendering on real-world data.
              Above we show quanlitative and quantitative results on the widely-used
              <a href="https://stanfordorb.github.io/">
                Stanford-ORB
              </a>
              dataset. Our model achieves reconstruction quality on par or even better than SOTA optimization-based methods that
              take dense views as inputs and several hours to run. It also achieves the highest geometry reconstruction quality compared to optimization-based methods.
              We observe that LIRM can better handle specular materials, creating specular highlights on the surface that can closely match those of ground-truth images,
              while optimization-based methods either miss the specular highlights or cannot match the ground-truths accurately.
              Since we encode background images as inputs, LIRM can also better separate lighting color from the material color, as shown in the second row.
              We also compare to concurrent LRM-based inverse rendering method MetaLRM. LIRM outperforms by a large margin both qualitatively and quantitatively.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <script>
    new BeforeAfter({
      id: '#example1'
    });
    new BeforeAfter({
      id: '#example2'
    });
    new BeforeAfter({
      id: '#example3'
    });
    new BeforeAfter({
      id: '#example4'
    });
    new BeforeAfter({
      id: '#example5'
    });
    new BeforeAfter({
      id: '#example6'
    });
  </script>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre>
        <code>
          @inproceedings{li2025lirm,
            title={LIRM: Large Inverse Rendering Model for Progressive Reconstruction of Shape, Materials and View-dependent Radiance Fields},
            author={Zhengqin Li and Dilin Wang and Ka Chen and Zhaoyang Lv and Thu Nguyen-Phuoc and Milim Lee and Jia-bin Huang
              and Lei Xiao and Cheng Zhang and Yufeng Zhu and Carl S. Marshall and Yufeng Ren and Richard Newcombe and Zhao Dong},
            journal={Proceedings of the IEEE conference on computer vision and pattern recognition},
            year={2025},
          }
        </code>
      </pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="content has-text-centered">
      </div>
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>
            <p>
              We thank the authors of <a href="https://nerfies.github.io/">Nerfies</a> and <a href="https://svraster.github.io/">SVRaster</a>
              that kindly open sourced the template of this website.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body></html>
